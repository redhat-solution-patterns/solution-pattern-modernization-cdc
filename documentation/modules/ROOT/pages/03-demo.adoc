= Solution Pattern: Using Change Data Capture for Stack Modernization
:sectnums:
:sectlinks:
:doctype: book
:page-toclevels: 3

This section brings information about the implementation of the solution pattern, how you can install it, try it out and check the details of the implementation with an actual running application.

== See the Solution in Action
// tag::demo[]
Here's a list of videos that you can use to explore this solution pattern.

* xref:03-demo.adoc#_see_an_overview_and_demonstration_of_this_solution_pattern[Solution Pattern Overview]
* xref:03-demo.adoc#_see_the_provisioning_in_action[How to provision this demo]
* xref:03-demo.adoc#_see_the_search_feature_in_action[The enhanced search capability in action]
* xref:03-demo.adoc#_see_the_cashback_wallet_in_action[The Cashback Wallet capability in action]

[#_see_an_overview_and_demonstration_of_this_solution_pattern]
See an overview and demonstration of this solution pattern:

Check below a twenty minutes explanation and demonstration of this solution pattern:

video::vTdP2mLXiHg[youtube, width=800, height=480]
// end::demo[]

== Run this demonstration

In order to try out this demonstration you will need to provision the environment. From an overall perspective, these are the steps to provision the demo:

1. Log in to OpenShift with `cluster-admin` role;
2. Run the https://github.com/solution-pattern-cdc/ansible.git[Ansible playbook] and enjoy the demo.

=== See the provisioning in action 

The video below demonstrates how to do the provisioning that is described in details the next sections. You can follow the provisioning steps as you follow the video. 

video::TvrbX4gKiv0[youtube, width=800, height=480]

=== Pre-requisites
==== Preparing the local environment 

Here is the list of tools you need in your machine to so you can use the automated installation.

TIP: For a better experience during provisioning and demo usage, it's recommended to have these CLI tools installed locally.

* https://docs.openshift.com/container-platform/4.12/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli[OpenShift CLI (oc client)]
* https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html[Ansible CLI] (_tested with v2.9_)
** Ansible https://docs.ansible.com/ansible/latest/collections/kubernetes/core/k8s_module.html[kubernetes.core] module

To check if you have the cli tools, you can open your terminal and use following commands:

[.console-input]
[source,shell script]
----
oc version #openshift cli client
ansible --version 
ansible-galaxy --version 
ansible-galaxy collection list #the list should include kubernetes.core
----

If you can't see `kubernetes.core` collection listed, you can install it with `ansible-galaxy`:

[.console-input]
[source,shell script]
----
$ ansible-galaxy collection install kubernetes.core
----

==== Preparing the platforms

* OpenShift cluster (version >= 4.12) with _cluster-admin_ privileges.
+
TIP: If you have access to RHDP, you can request and use an `OpenShift 4.12 Workshop` enviroment.

=== Provisioning the demo

The solution's components and services can be automatically provisioned using an Ansible playbook.

The following steps will guide you on setting up an instance of OpenShift Streams for Apache Kafka and its resources, plus provisioning the demo services using Ansible.
[#cli-tools]


==== Installing the demo

This solution pattern offers an easy installation process through Ansible automation, Red hat OpenShift Gitops and Helm charts. To get your environment up and running, follow the steps below:

1. Clone the repository below to your workstation
+
[.console-input]
[source,shell script]
----
git clone https://github.com/solution-pattern-cdc/ansible.git
cd ansible
----

2. Run the Ansible playbook:
+
[.console-input]
[source,shell script]
----
ansible-playbook -playbooks/install.yml
----

Once the playbook finished successfully, you should be able to see the different components of the demo installed in the `retail` namespace on your OpenShift cluster.

To check if your environment is healthy:

1. Access your OpenShift console, and using the Administrator view, on the left menu select *Workloads -> Deployments*;
2. All services should be healthy, like displayed below:
+
image::03/ocp_pods_running.png[]

=== Accessing the services

You can access the three services that expose a UI through the exposed routes. Use one of the two options below to get the routes:

a. Using `oc cli`, copy and paste the whole command below:
+
[.console-input]
[source,shell script]
----
cat << EOF
========================================
Kafdrop: https://$(oc get route kafdrop --template='{{ .spec.host }}' -n retail) 
Search service: https://$(oc get route search-service --template='{{ .spec.host }}' -n retail) 
Simulation Service: https://$(oc get route retail-simulation --template='{{ .spec.host }}' -n retail)/q/swagger-ui 
Cashback Wallet UI: https://$(oc get route cashback-service-ui --template='{{ .spec.host }}' -n retail)
========================================
EOF
----
+
b. Using the OpenShift console:
+    
image::03/ocp_routes.png[]

== Walkthrough guide

A retail store specialized in plants wants to grow its market by expanding in the online market. To do so, they need to start the adoption of new technologies without impacting the existing application that is currently running in production. All information about sales, customers and products are still maintained via legacy application, but this data is also required by the new capabilities.

Two new functionalities are now part of the retail solution:

1. Enhanced search capabilities for products
2. Cashback wallet for customers

Both solutions are build on top of an event driven architecture, which means that all services are integrated with an orchestration where each one execute its own operations when relevant events are published in the ecosystem.  

Let's see both solutions in action, starting with the new search capabilities.

=== Enhanced search capabilities for products

To test the enhanced search capabilities, we will:

1. Use the *search service* to see existing data that is available in the ElasticSearch index;
2. Add a new product directly to the *retail database* (legacy), to check the ecosystem behavior;
3. Confirm that the new product shows up in the search;
4. Check the events that were published in order for the synchronization to happen;
5. Delete the product directly on the retail database; 
6. Confirm that the product no longer shows up in the *search service*.

==== See the search feature in action

In this video you can see the working implementation of the new enhanced search capabilities:

video::C90x_utWQkk[youtube, width=800, height=480]

==== Trying out the new enhanced search

1. Using your browser, open the *search service*.
+
NOTE: You can get the URL as described in the section xref:03-demo.adoc#cli-tools[obtaining the services URL].
+
2. In the search field, search for "*yellow*". You should see several results.
+
image::03/search-service-result-yellow.png[]
+
3. Next, search for "kopi" or "java". No result will show up.
4. Let's insert a new product directly in the *retail database* and see if it will reflect on this service. Use the console inside the *retail-db* container. You can either access the container using your browser, accessing the OpenShift Console (*Workloads -> Pods -> retail-db-XXXX -> Terminal*);
+
Or by using your terminal as shown below:
+
[.console-input]
[source,shell script]
----
oc project retail
oc rsh deployment/retail-db
----
5. Next, inside the container, we will access postgres, connect to the *retail* database and check the structure of the *product* table:
+
[.console-input]
[source,shell script]
----
psql
\c retail
\d product
----
+
As we can see, a product has an `id`, `name`, `description` and a `price`.
+
image::03/retail-db-terminal.png[]

6. Let's add a new product in this table, the product "*Kopi luwak*": 
+
[.console-input]
[source,sql]
----
insert into public.product (product_id, name, price, description) values (7777, 'Kopi luwak', 20, 'Kopi luwak is a coffee that consists of partially digested coffee cherries, which have been eaten and defecated by the Asian palm civet (Paradoxurus hermaphroditus). It is produced mainly on the Indonesian islands of Sumatra, Java, Bali, Sulawesi, and in East Timor.');
----

7. Now, as required by the use case, even though this data was changed in the legacy database, it should be available for search in the new services. Let's confirm that this change was reflected in the ElasticSearch products index.
+
Open the *search-service* application in your browser and search for "java" or "kopi". You should be able to see your new product.  
+
image::03/search-service-result-java.png[]

Let's delete this product from the retail database to validate if delete operations are also being tracked. 

1. In the *retail-db* container terminal, now execute the following SQL:
+
[.console-input]
[source,sql]
----
DELETE FROM public.product where product_id = 7777;
----

2. Go back to the *search-service* in your browser, and search for '*kopi*' or '*java*' again. 

==== Looking behind the scenes - enhanced search

It's now time to take a look at how the system is working in order to allow this capability to work as we have seen.

The components of the search capability we have just tried are:

[cols="28m,^.^13,~"]
[frame=all, grid=all]
|===
|*Service* | *Type* | *Description* 
| retail-db
| PostgreSQL database used by the legacy services
| Persistence

|kafka-connect-connect
.2+| Integration
| Kafka connectors for database event streaming (debezium);

|elastic-connector
|Camel + Quarkus service for event-driven synchronization of product data with ElasticSearch;

|kafdrop
.2+| Data Visualization
|a kafka client ui to facilitate the visualization of events and topics;

|search-service
|Quarkus + ElasticSearch extension to simplify the visualization of the indexed data residing in elastic search;
|===

NOTE: If you go to your OpenShift, you should be able to see one `deployment` resource for each of the above services.

*So, how was the new product added to the ElasticSearch index?*


1. A new product is created in the *retail.product* table, in the legacy database *retail-db*;
2. xref:appendix-a.adoc#_kafka_connect__debezium_installation[Debezium] tracks it and publishes the events it to topics in Apache Kafka;
3. The *elastic-connector*, implemented with Camel and Quarkus is subscribed to the topic mentioned above. It processes the event data and pushes the *product name* and *description* to an ElasticSearch index:

.Partial code - processing logic in the https://github.com/solution-pattern-cdc/elastic-connector/blob/main/src/main/java/org/acme/retail/ProductRoute.java[`ProductRoute`]

[source,java]
----
(...)
    .process(exchange -> {
        Message in = exchange.getIn();
        JsonObject after = new JsonObject(in.getBody(Map.class)).getJsonObject("after");
        Map<String, String> document = new HashMap<>();
        document.put("name", after.getString("name"));
        document.put("description", after.getString("description"));
        IndexRequest request = new IndexRequest(in.getHeader(ElasticsearchConstants.PARAM_INDEX_NAME, String.class))
                .id(String.valueOf(after.getLong("product_id"))).source(document);
        in.setBody(request);
    })
(...)
----

This flow can be represented like this:

image::03/arch_search.png[]

=== Cashback Wallet functionality

Now, let's see more ways we can explore CDC to add new capabilities to our existing stack. Since we have all the new sales being streamed as events, we can use it to build the new cashback wallet service.

To walk through this demonstration, you will need to access the following services in your browser:

* cashback-service-ui
* kafdrop
* simulation service Swagger-UI

=== See the Cashback Wallet in action 

The following video shows the working implementation of the new cashback wallet capabilities:

video::W813zm5qG2Q[youtube, width=800, height=480]

==== Trying out the new cashback wallet 

1. Open the *cashback-service-ui*:
+
NOTE: You can get the URL as described in the section xref:03-demo.adoc#cli-tools[obtaining the services URL].

2. You should be able to see a list of cashback wallets and its data:
+
image::03/cashback-wallet-clean.png[]

3. Choose one of the customers in that list that has no cashback. It will be easier to see the new cashback credits. You can see the customer ID in the beginning of the line:
+
image::03/cashback-wallet-customer-id.png[]

4. Next, we will simulate as if a customer has purchased five items in the store. In your browser open the *simulation service* swagger-ui, (service-url/q/swagger-ui).
+
image::03/simulate-purchase.png[]
+ 
5. Click on *try it out*, input the customer ID you have chosen, and submit the request. This will generate five purchases for this customer. 
+
image::03/simulate-purchase-result.png[]

6. You should get an HTTP 200 result. In the legacy system, the purchases are stored in two different tables, the `retail.sale` and `retail.line_item`. So if you simulate five sales, the data will be stored in both tables and streamed as events by Debezium to two respective topics. 
+
Through a series of orchestrated operations, the data will be aggregated, processed, and enriched (`sales-aggregated` service), to finally be used to calculate and update the cashback wallet's values (`cashback-service`).

7. Open Kafdrop in your browser.

8. Locate and click on the topic `retail.sale-aggregated`, and then, click on *view messages*. This is the result of the Kafka Streams (*sales-stream* service) operations of aggregation, processing and enrichment of the events' data that were streamed by Debezium:
+
image::03/kafdrop-sales-aggregated-messages.png[]
+
NOTE: To see a detailed explanation about the events processing refer to the xref:_looking_behind_the_scenes__cashback_solution[Looking behind the scenes] section.

9. Open the Cashback Wallet in your browser and refresh the page. You should be able to check the new earned cashback for each purchase of your customer!
+
image::03/cashback-wallet-complete.png[]

See below a diagram that represents the orchestration processing that just happened when you simulated new purchases and saw the respective incoming cashback:

image::03/arch-cashback.png[]

==== Looking behind the scenes - cashback solution

Differently than the search capability that only requires the integration layer (Retail DB -> ElasticSearch), to create cashback wallets we'll need to process and enrich the data before we use it. We will also need to guarantee the synchronization between the customer data in the *retail-db* and the *cashback-db*.

1. When a new sale is registered, new lines are created in the `retail.sale` and `retail.line_item` tables. 
2. Debezium then tracks and publishes events to *two topics*, one for each respective table, and one event for each respective line added/updated event that was tracked. But notice that in order for us to apply the cashback calculation business logic, we'll have in mind good design and architecture practices for microservices, where each microservice _is supposed to do one thing, and do it well_. So, the event data aggregation, processing and enrichment will be executed by a service (*sales-streams*) before we actually do the cashback operations in another service (*cashback-service*); 
+
Here's another way to explain this:
+
* if someone buys two cactus and one lilly in the same purchase, there will be two line_items registered for a single sale. See below the tables structures:
+
[source,sql]
----
$ oc rsh deployment/retail-db #<1>
sh-4.4$ psql #<2>
psql (12.5)
Type "help" for help.

postgres=# \c retail #<3>
You are now connected to database "retail" as user "postgres".
retail=# select * from sale;  #<4>
sale_id | customer_id |          date
---------+-------------+-------------------------
1000 |        1000 | 2022-06-03 20:27:57.66
1001 |        1000 | 2022-06-03 20:27:57.767
1002 |        1000 | 2022-06-03 20:27:57.852
1003 |        1000 | 2022-06-03 20:27:57.854
1004 |        1000 | 2022-06-03 20:27:57.857
(5 rows)

retail=# select * from line_item; #<5>
line_item_id | sale_id | product_id | price  | quantity
--------------+---------+------------+--------+----------
1000 |    1000 |        198 |  99.40 |        2
1001 |    1000 |        851 |  72.97 |        3
1002 |    1000 |         87 |  66.19 |        3
1003 |    1000 |        243 |  83.20 |        1
1004 |    1001 |         80 | 127.56 |        3
1005 |    1001 |        639 | 193.80 |        1
1006 |    1002 |        563 | 156.08 |        3
1007 |    1003 |        532 |  89.98 |        3
1008 |    1003 |        374 |  87.17 |        1
1009 |    1003 |        932 |  32.69 |        3
1010 |    1003 |        662 | 141.31 |        3
1011 |    1003 |        304 |  39.84 |        1
1012 |    1004 |        138 | 125.81 |        3
1013 |    1004 |        656 | 103.99 |        3
1014 |    1004 |        285 | 168.79 |        3
1015 |    1004 |         84 | 113.79 |        2
(16 rows)
----
<1> Use `oc-client` to access the `retail-db` container;
<2> Access PostgreSQL from within the container;
<3> Connect to the retail database;
<4> List all the sales;
<5> List all the items of the sales;
+
* Debezium will stream each change individually, which results with several events in two topics, one of each table.
* But, when we calculate the earned cashback for the sale, we use the total amount of the sale - the sum of all the line items of that sale.
* Using https://developers.redhat.com/learn/openshift-streams-for-apache-kafka/guided-workshop-for-kafka-streams/what-is-kafka-streams[*Kafka Streams*], the `sales-aggregated` service aggregates, processes and enriches the events' data.
+
.Partial code in the `Sales-Streams` service used to aggregate and enrich data;
[source,java] 
----
// Join LineItem events with sale events by foreign key, aggregate Linetem price in sale
KTable<Long, AggregatedSale> aggregatedSales = lineItemTable
        .join(saleTable, lineItem -> lineItem.sale,
                (lineItem, sale) -> new SaleAndLineItem(sale, lineItem),
                Materialized.with(Serdes.Long(), saleAndLineItemSerde))
        .groupBy((key, value) -> KeyValue.pair(value.sale.saleId, value), Grouped.with(Serdes.Long(), saleAndLineItemSerde))
        .aggregate(AggregatedSale::new, (key, value, aggregate) -> aggregate.addLineItem(value),
                (key, value, aggregate) -> aggregate.removeLineItem(value),
                Materialized.with(Serdes.Long(), aggregatedSaleSerde));

aggregatedSales.toStream().to(aggregatedSaleTopic, Produced.with(Serdes.Long(), aggregatedSaleSerde));
----

3. Next, if you go back to the homepage of Kafdrop, open *`retail.expense-event` -> view messages -> view messages*; The *sales-streams* service to notify the ecosystem that new processed information is available by publishing events on the `expense-event` topic. 
+
Let's see the result of this processing with Kafdrop.
+
image::03/kafdrop-expense-event.png[]
+
Based on these events published in the `expense-event` topic, services like the *cashback-service* can react and use the event data to handle the cashback business logic operations. 
+ 
NOTE: See how the values are calculated and persisted in the https://github.com/solution-pattern-cdc/cashback-service/blob/main/src/main/java/org/acme/cashback/processor/ValuesProcessor.java[cashback values processor] in the `cashback-service`
+ 
* Let's take a look over the cashback service processing:
+
.Partial code implementation in the https://github.com/solution-pattern-cdc/cashback-service/blob/e116b0b0f8067c1a69298e6e4b214224c0d3e1b6/src/main/java/org/acme/cashback/route/CashbackRoute.java[Cashback Route] in the cashback-service.
[source,java]
----
        from("kafka:{{kafka.expenses.topic.name}}?groupId={{kafka.cashback_processor.consumer.group}}" + #<1>
                "&autoOffsetReset=earliest")
                .routeId("CashbackProcessor") 
                .unmarshal(new JacksonDataFormat(ExpenseEvent.class))
                .setHeader("operation", simple("${body.operation}")) #<2>
                .setHeader("sale_id", simple("${body.saleId}")) #<2>
                .to("direct:filterInvalidOperationCodes") #<3>
                .to("direct:getData") #<4>
                .to("direct:filterInvalidData") #<5>
                .choice()
                .when().simple("${header.operation} == 'c'").log(LoggingLevel.DEBUG,"Processing create event") #<6>
                    .process("valuesProcessor")
                    .choice()
                        .when(simple("${body.cashbackId} == null"))
                            .log(LoggingLevel.DEBUG, "No cashback wallet exists. Creating new cashback for: ${body}")
                            .to("direct:createAndPersistCashback") 
                    .end()
                    .to("direct:updateEarnedCashbackData")
                .endChoice()
                .otherwise().when().simple("${header.operation}== 'u'").log(LoggingLevel.DEBUG,"Processing update event") #<7>
                    .process("valuesProcessor")
                    .to("direct:updateEarnedCashbackData")
                .end();
----
<1> Consumed topic with name configured in the property `kafka.expenses.topic.name`;
<2> Sets incoming information in the message header;
<3> Filter out incoming operations that are not `create` and `update`;  
<4> Retrieves existing customer and cashback information from the local database for the incoming sale;
<5> Filter out information for incoming data that is invalid - is not in the cashback database;
<6> When a new expense "create" event is received, the service checks if the customer already has a wallet - if not, creates one. Then, it updates the cashback wallet values calculated and persisted.  
<7> If the incoming operation is "update", then, a new wallet does not need to be created. The values are calculated and updated.


== Conclusion 

In this section you have learned how to:

. Provision the demo environment;
. How to try out and check how CDC enables the delivery of the demo implementation:
.. How a new search index technology could be added to the existing solution and enable enhanced search capabilities for legacy data;
.. How a whole new cashback wallet capability could be added without impacting the legacy systems by using a distributed, event-driven and microservice-based architecture; 
. Learn in-depth details about services can be orchestrated;

// tag::conclusion[]
The solution is built on top of a hybrid cloud model, with containerized services running on OpenShift (can be on a private or public cloud depending on how you provision the demo), using an Apache Kafka broker cluster running in the same OpenShift instance.

This design is only possible by the designing the architecture based on the Change Data Capture pattern - which was delivered with Debezium and Kafka Connectors.
// end::conclusion[]